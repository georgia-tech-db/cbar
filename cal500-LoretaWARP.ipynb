{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Quickstart guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to build a simple content-based audio retrieval model and evaluate the retrieval accuracy on a small song dataset, CAL500. This dataset consists of 502 western pop songs, performed by 499 unique artists. Each song is tagged by at least three people using a standard survey and a fixed tag vocabulary of 174 musical concepts.\n",
    "\n",
    "This package includes a loading utility for getting and processing this dataset, which makes loading quite easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbar.datasets import fetch_cal500\n",
    "\n",
    "X, Y = fetch_cal500()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `fetch_cal500()` initally downloads the CAL500 dataset to a subfolder of your home directory. You can specify a different location using the `data_home` parameter (`fetch_cal500(data_home='path')`). Subsequents calls simply load the dataset.\n",
    "\n",
    "The raw dataset consists of about 10,000 39-dimensional features vectors\n",
    "per minute of audio content which were created by\n",
    "\n",
    "1. Sliding a half-overlapping short-time window of 12 milliseconds over each song's waveform data.\n",
    "2. Extracting the 13 mel-frequency cepstral coefficients.\n",
    "3. Appending the instantaneous first-order and second-order derivatives.\n",
    "\n",
    "Each song is, then, represented by exactly 10,000 randomly subsampled, real-valued feature vectors as a *bag-of-frames*. The *bag-of-frames* features are further processed into one *k*-dimensional feature vector by encoding the feature vectors using a codebook and pooling them into one compact vector.\n",
    "\n",
    "Specifically, *k*-means is used to cluster all frame vectors into *k* clusters. The resulting cluster centers correspond to the codewords in the codebook. Each frame vector is assigned to its closest cluster center and a song represented as the counts of frames assigned to each of the *k* cluster centers.\n",
    "\n",
    "By default, `fetch_cal500()` uses a codebook size of 512 but this size is easily modified with the `codebook_size` parameter (`fetch_cal500(codebook_size=1024)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((502, 512), (502, 174))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training data and test data, fit the model on the training data, and evaluate it on the test data. Import and instantiate the model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbar.loreta import LoretaWARP\n",
    "\n",
    "model = LoretaWARP(n0=0.1, valid_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then split the data and fit the model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:        0, stepsize:    0.100, P10: 0.168, AP: 0.165, loss: 0.000\n",
      "iter:     1000, stepsize:    0.100, P10: 0.177, AP: 0.193, loss: 1.000\n",
      "iter:     2000, stepsize:    0.100, P10: 0.201, AP: 0.198, loss: 0.993\n",
      "iter:     3000, stepsize:    0.100, P10: 0.195, AP: 0.198, loss: 0.978\n",
      "iter:     4000, stepsize:    0.100, P10: 0.195, AP: 0.200, loss: 0.965\n",
      "iter:     5000, stepsize:    0.100, P10: 0.192, AP: 0.198, loss: 0.954\n",
      "iter:     6000, stepsize:    0.100, P10: 0.193, AP: 0.197, loss: 0.949\n",
      "iter:     7000, stepsize:    0.100, P10: 0.192, AP: 0.197, loss: 0.933\n",
      "iter:     8000, stepsize:    0.100, P10: 0.175, AP: 0.195, loss: 0.919\n",
      "iter:     9000, stepsize:    0.100, P10: 0.187, AP: 0.195, loss: 0.919\n",
      "iter:    10000, stepsize:    0.100, P10: 0.183, AP: 0.194, loss: 0.896\n",
      "iter:    11000, stepsize:    0.100, P10: 0.188, AP: 0.196, loss: 0.895\n",
      "iter:    12000, stepsize:    0.100, P10: 0.189, AP: 0.196, loss: 0.900\n",
      "iter:    13000, stepsize:    0.100, P10: 0.189, AP: 0.194, loss: 0.894\n",
      "2019-04-09 04:06:25,567 [MainThread  ] [WARNI]  max_dips reached, stopped at 14000 iterations.\n",
      "CPU times: user 5min 4s, sys: 16min 23s, total: 21min 28s\n",
      "Wall time: 23.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cbar.loreta.LoretaWARP(max_iter=100000, k=30, n0=0.1, n1=0.0, rank_thresh=0.1, lambda_=0.1, loss='warp', max_dips=10, valid_interval=1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cbar.cross_validation import train_test_split_plus\n",
    "\n",
    "(X_train, X_test,\n",
    " Y_train, Y_test,\n",
    " Q_vec, weights) = train_test_split_plus(X, Y)\n",
    "\n",
    "%time model.fit(X_train, Y_train, Q_vec, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, predict the scores for each query with all songs. Ordering the songs from highest to lowest score corresponds to the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_score = model.predict(Q_vec, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: [0.18292682926829268],\n",
       "             2: [0.16768292682926828],\n",
       "             3: [0.1727642276422764],\n",
       "             4: [0.1717479674796748],\n",
       "             5: [0.1693089430894309],\n",
       "             6: [0.1660569105691057],\n",
       "             7: [0.169904181184669],\n",
       "             8: [0.17531213704994195],\n",
       "             9: [0.18318815331010452],\n",
       "             10: [0.18567799070847857],\n",
       "             11: [0.19069308063210505],\n",
       "             12: [0.18736097912927183],\n",
       "             13: [0.19493084828450685],\n",
       "             14: [0.19983123245318368],\n",
       "             15: [0.20638127116785654],\n",
       "             16: [0.21725246298417034],\n",
       "             17: [0.22199265195588724],\n",
       "             18: [0.2300584762161161],\n",
       "             19: [0.23003708126532452],\n",
       "             20: [0.2357371898858439]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cbar.evaluation import Evaluator\n",
    "from cbar.utils import make_relevance_matrix\n",
    "\n",
    "n_relevant = make_relevance_matrix(Q_vec, Y_train).sum(axis=1)\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.eval(Q_vec, weights, Y_score, Y_test, n_relevant)\n",
    "evaluator.prec_at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "The `cv` function in the `cross_validation` module offers an easy way to evaluate a retrieval method on multiple splits of the data. Let's run the same experiment on three folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbar.cross_validation import cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-09 04:06:25,780 [MainThread  ] [INFO ]  Running CV with 3 folds ...\n",
      "2019-04-09 04:06:43,751 [MainThread  ] [INFO ]  Validating fold 0 ...\n",
      "iter:        0, stepsize:    0.100, P10: 0.160, AP: 0.155, loss: 0.000\n",
      "iter:     1000, stepsize:    0.100, P10: 0.192, AP: 0.189, loss: 0.999\n",
      "iter:     2000, stepsize:    0.100, P10: 0.200, AP: 0.190, loss: 0.988\n",
      "iter:     3000, stepsize:    0.100, P10: 0.190, AP: 0.182, loss: 0.976\n",
      "iter:     4000, stepsize:    0.100, P10: 0.180, AP: 0.178, loss: 0.959\n",
      "iter:     5000, stepsize:    0.100, P10: 0.177, AP: 0.177, loss: 0.940\n",
      "iter:     6000, stepsize:    0.100, P10: 0.175, AP: 0.180, loss: 0.931\n",
      "iter:     7000, stepsize:    0.100, P10: 0.176, AP: 0.177, loss: 0.918\n",
      "iter:     8000, stepsize:    0.100, P10: 0.170, AP: 0.178, loss: 0.899\n",
      "iter:     9000, stepsize:    0.100, P10: 0.177, AP: 0.175, loss: 0.889\n",
      "iter:    10000, stepsize:    0.100, P10: 0.172, AP: 0.175, loss: 0.878\n",
      "iter:    11000, stepsize:    0.100, P10: 0.165, AP: 0.174, loss: 0.870\n",
      "2019-04-09 04:07:04,166 [MainThread  ] [WARNI]  max_dips reached, stopped at 12000 iterations.\n",
      "2019-04-09 04:07:04,377 [MainThread  ] [INFO ]  Validating fold 1 ...\n",
      "iter:        0, stepsize:    0.100, P10: 0.182, AP: 0.153, loss: 0.000\n",
      "iter:     1000, stepsize:    0.100, P10: 0.159, AP: 0.174, loss: 0.998\n",
      "iter:     2000, stepsize:    0.100, P10: 0.166, AP: 0.175, loss: 0.990\n",
      "iter:     3000, stepsize:    0.100, P10: 0.158, AP: 0.173, loss: 0.973\n",
      "iter:     4000, stepsize:    0.100, P10: 0.163, AP: 0.172, loss: 0.959\n",
      "iter:     5000, stepsize:    0.100, P10: 0.164, AP: 0.173, loss: 0.942\n",
      "iter:     6000, stepsize:    0.100, P10: 0.167, AP: 0.172, loss: 0.929\n",
      "iter:     7000, stepsize:    0.100, P10: 0.167, AP: 0.172, loss: 0.907\n",
      "iter:     8000, stepsize:    0.100, P10: 0.166, AP: 0.172, loss: 0.895\n",
      "iter:     9000, stepsize:    0.100, P10: 0.163, AP: 0.173, loss: 0.882\n",
      "iter:    10000, stepsize:    0.100, P10: 0.161, AP: 0.172, loss: 0.868\n",
      "iter:    11000, stepsize:    0.100, P10: 0.168, AP: 0.170, loss: 0.875\n",
      "2019-04-09 04:07:22,896 [MainThread  ] [WARNI]  max_dips reached, stopped at 12000 iterations.\n",
      "2019-04-09 04:07:23,105 [MainThread  ] [INFO ]  Validating fold 2 ...\n",
      "iter:        0, stepsize:    0.100, P10: 0.152, AP: 0.156, loss: 0.000\n",
      "iter:     1000, stepsize:    0.100, P10: 0.165, AP: 0.174, loss: 0.999\n",
      "iter:     2000, stepsize:    0.100, P10: 0.170, AP: 0.174, loss: 0.992\n",
      "iter:     3000, stepsize:    0.100, P10: 0.173, AP: 0.176, loss: 0.984\n",
      "iter:     4000, stepsize:    0.100, P10: 0.171, AP: 0.180, loss: 0.960\n",
      "iter:     5000, stepsize:    0.100, P10: 0.171, AP: 0.177, loss: 0.944\n",
      "iter:     6000, stepsize:    0.100, P10: 0.160, AP: 0.176, loss: 0.924\n",
      "iter:     7000, stepsize:    0.100, P10: 0.162, AP: 0.173, loss: 0.921\n",
      "iter:     8000, stepsize:    0.100, P10: 0.158, AP: 0.174, loss: 0.903\n",
      "iter:     9000, stepsize:    0.100, P10: 0.155, AP: 0.173, loss: 0.887\n",
      "iter:    10000, stepsize:    0.100, P10: 0.154, AP: 0.170, loss: 0.862\n",
      "iter:    11000, stepsize:    0.100, P10: 0.156, AP: 0.172, loss: 0.853\n",
      "iter:    12000, stepsize:    0.100, P10: 0.146, AP: 0.170, loss: 0.866\n",
      "iter:    13000, stepsize:    0.100, P10: 0.153, AP: 0.171, loss: 0.840\n",
      "2019-04-09 04:07:47,772 [MainThread  ] [WARNI]  max_dips reached, stopped at 14000 iterations.\n",
      "CPU times: user 14min 17s, sys: 44min 47s, total: 59min 5s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%time cv('cal500', 512, n_folds=3, method='loreta', n0=0.1, valid_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation results including retrieval method parameters are written to a JSON file. For each dataset three separate result files for mean average precision (MAP), precision-at-*k*, and precision-at-10 as a function of relevant training examples are written to disk. Here are the mean average precision values of the last cross-validation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17251897309918962, 0.17584143541984665, 0.17753117399618143]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from cbar.settings import RESULTS_DIR\n",
    "\n",
    "results = json.load(open(os.path.join(RESULTS_DIR, 'cal500_mean_ap.json')))\n",
    "results[list(results.keys())[-1]]['precision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start cross-validation with the CLI\n",
    "\n",
    "This package comes with a simple CLI which makes it easy to start cross-validation experiments from the command line. The CLI enables you to specify a dataset and a retrieval method as well as additional options in one line.\n",
    "\n",
    "To start an experiment on the CAL500 dataset with the LORETA retrieval method, use the following command.\n",
    "\n",
    "```\n",
    "$ cbar crossval --dataset cal500 loreta \n",
    "```\n",
    "\n",
    "This simple command uses all the default parameters for LORETA but you can specify all parameters as arguments to the `loreta` command. To see the available options for the `loreta` command, ask for help like this.\n",
    "\n",
    "```\n",
    "$ cbar crossval loreta --help\n",
    "Usage: cbar crossval loreta [OPTIONS]\n",
    "\n",
    "Options:\n",
    "  -n, --max-iter INTEGER        Maximum number of iterations\n",
    "  -i, --valid-interval INTEGER  Rank of parameter matrix W\n",
    "  -k INTEGER                    Rank of parameter matrix W\n",
    "  --n0 FLOAT                    Step size parameter 1\n",
    "  --n1 FLOAT                    Step size parameter 2\n",
    "  -t, --rank-thresh FLOAT       Threshold for early stopping\n",
    "  -l, --lambda FLOAT            Regularization constant\n",
    "  --loss [warp|auc]             Loss function\n",
    "  -d, --max-dips INTEGER        Maximum number of dips\n",
    "  -v, --verbose                 Verbosity\n",
    "  --help                        Show this message and exit.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
