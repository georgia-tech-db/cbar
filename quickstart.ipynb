{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Quickstart guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to build a simple content-based audio retrieval model and evaluate the retrieval accuracy on a small song dataset, CAL500. This dataset consists of 502 western pop songs, performed by 499 unique artists. Each song is tagged by at least three people using a standard survey and a fixed tag vocabulary of 174 musical concepts.\n",
    "\n",
    "This package includes a loading utility for getting and processing this dataset, which makes loading quite easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbar.datasets import fetch_cal500\n",
    "\n",
    "X, Y = fetch_cal500()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `fetch_cal500()` initally downloads the CAL500 dataset to a subfolder of your home directory. You can specify a different location using the `data_home` parameter (`fetch_cal500(data_home='path')`). Subsequents calls simply load the dataset.\n",
    "\n",
    "The raw dataset consists of about 10,000 39-dimensional features vectors\n",
    "per minute of audio content which were created by\n",
    "\n",
    "1. Sliding a half-overlapping short-time window of 12 milliseconds over each song's waveform data.\n",
    "2. Extracting the 13 mel-frequency cepstral coefficients.\n",
    "3. Appending the instantaneous first-order and second-order derivatives.\n",
    "\n",
    "Each song is, then, represented by exactly 10,000 randomly subsampled, real-valued feature vectors as a *bag-of-frames*. The *bag-of-frames* features are further processed into one *k*-dimensional feature vector by encoding the feature vectors using a codebook and pooling them into one compact vector.\n",
    "\n",
    "Specifically, *k*-means is used to cluster all frame vectors into *k* clusters. The resulting cluster centers correspond to the codewords in the codebook. Each frame vector is assigned to its closest cluster center and a song represented as the counts of frames assigned to each of the *k* cluster centers.\n",
    "\n",
    "By default, `fetch_cal500()` uses a codebook size of 512 but this size is easily modified with the `codebook_size` parameter (`fetch_cal500(codebook_size=1024)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((502, 512), (502, 174))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training data and test data, fit the model on the training data, and evaluate it on the test data. Import and instantiate the model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbar.pamir import PAMIR\n",
    "\n",
    "model = PAMIR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then split the data and fit the model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rocket/miniconda3/envs/cbar/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:        0, P10: 0.193, AP: 0.189, loss: 0.000\n",
      "iter:    10000, P10: 0.196, AP: 0.192, loss: 0.915\n",
      "iter:    20000, P10: 0.182, AP: 0.192, loss: 0.629\n",
      "iter:    30000, P10: 0.180, AP: 0.190, loss: 0.516\n",
      "iter:    40000, P10: 0.191, AP: 0.193, loss: 0.434\n",
      "iter:    50000, P10: 0.186, AP: 0.195, loss: 0.393\n",
      "iter:    60000, P10: 0.179, AP: 0.193, loss: 0.349\n",
      "iter:    70000, P10: 0.185, AP: 0.193, loss: 0.322\n",
      "iter:    80000, P10: 0.183, AP: 0.194, loss: 0.295\n",
      "iter:    90000, P10: 0.190, AP: 0.194, loss: 0.276\n",
      "CPU times: user 2min 15s, sys: 1min 21s, total: 3min 37s\n",
      "Wall time: 1min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cbar.pamir.PAMIR(max_iter=100000, C=1.0, valid_interval=10000, max_dips=20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cbar.cross_validation import train_test_split_plus\n",
    "\n",
    "(X_train, X_test,\n",
    " Y_train, Y_test,\n",
    " Q_vec, weights) = train_test_split_plus(X, Y)\n",
    "\n",
    "%time model.fit(X_train, Y_train, Q_vec, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, predict the scores for each query with all songs. Ordering the songs from highest to lowest score corresponds to the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_score = model.predict(Q_vec, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: [0.19512195121951223],\n",
       "             2: [0.18902439024390244],\n",
       "             3: [0.19512195121951217],\n",
       "             4: [0.17784552845528456],\n",
       "             5: [0.17479674796747968],\n",
       "             6: [0.1719512195121951],\n",
       "             7: [0.1784843205574913],\n",
       "             8: [0.18106126596980254],\n",
       "             9: [0.17843592721641502],\n",
       "             10: [0.17957801006581497],\n",
       "             11: [0.1819165082884595],\n",
       "             12: [0.18620635800513852],\n",
       "             13: [0.18959128608518855],\n",
       "             14: [0.1965277270155319],\n",
       "             15: [0.21007529056309546],\n",
       "             16: [0.213417908311201],\n",
       "             17: [0.21588297970561243],\n",
       "             18: [0.2227166952687039],\n",
       "             19: [0.23173199265851988],\n",
       "             20: [0.23776231236021314]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cbar.evaluation import Evaluator\n",
    "from cbar.utils import make_relevance_matrix\n",
    "\n",
    "n_relevant = make_relevance_matrix(Q_vec, Y_train).sum(axis=1)\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.eval(Q_vec, weights, Y_score, Y_test, n_relevant)\n",
    "evaluator.prec_at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "The `cv` function in the `cross_validation` module offers an easy way to evaluate a retrieval method on multiple splits of the data. Let's run the same experiment on three folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbar.cross_validation import cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-13 15:00:29,314 [MainThread  ] [INFO ]  Running CV with 3 folds ...\n",
      "2019-03-13 15:00:52,956 [MainThread  ] [INFO ]  Validating fold 1 ...\n",
      "iter:        0, P10: 0.162, AP: 0.172, loss: 0.000\n",
      "iter:     1000, P10: 0.169, AP: 0.170, loss: 1.249\n",
      "iter:     2000, P10: 0.168, AP: 0.172, loss: 1.106\n",
      "iter:     3000, P10: 0.166, AP: 0.175, loss: 1.071\n",
      "iter:     4000, P10: 0.173, AP: 0.176, loss: 0.909\n",
      "iter:     5000, P10: 0.178, AP: 0.177, loss: 0.899\n",
      "iter:     6000, P10: 0.175, AP: 0.178, loss: 0.885\n",
      "iter:     7000, P10: 0.177, AP: 0.177, loss: 0.682\n",
      "iter:     8000, P10: 0.175, AP: 0.179, loss: 0.673\n",
      "iter:     9000, P10: 0.170, AP: 0.179, loss: 0.654\n",
      "iter:    10000, P10: 0.170, AP: 0.179, loss: 0.716\n",
      "iter:    11000, P10: 0.173, AP: 0.180, loss: 0.640\n",
      "iter:    12000, P10: 0.174, AP: 0.180, loss: 0.556\n",
      "iter:    13000, P10: 0.173, AP: 0.177, loss: 0.628\n",
      "iter:    14000, P10: 0.172, AP: 0.177, loss: 0.554\n",
      "iter:    15000, P10: 0.166, AP: 0.177, loss: 0.574\n",
      "iter:    16000, P10: 0.168, AP: 0.176, loss: 0.566\n",
      "iter:    17000, P10: 0.166, AP: 0.176, loss: 0.542\n",
      "iter:    18000, P10: 0.169, AP: 0.176, loss: 0.569\n",
      "iter:    19000, P10: 0.174, AP: 0.176, loss: 0.508\n",
      "iter:    20000, P10: 0.174, AP: 0.176, loss: 0.501\n",
      "iter:    21000, P10: 0.178, AP: 0.177, loss: 0.469\n",
      "iter:    22000, P10: 0.180, AP: 0.178, loss: 0.498\n",
      "iter:    23000, P10: 0.182, AP: 0.177, loss: 0.476\n",
      "iter:    24000, P10: 0.182, AP: 0.178, loss: 0.429\n",
      "iter:    25000, P10: 0.187, AP: 0.177, loss: 0.450\n",
      "iter:    26000, P10: 0.183, AP: 0.177, loss: 0.447\n",
      "iter:    27000, P10: 0.175, AP: 0.176, loss: 0.453\n",
      "iter:    28000, P10: 0.179, AP: 0.177, loss: 0.470\n",
      "iter:    29000, P10: 0.179, AP: 0.177, loss: 0.443\n",
      "iter:    30000, P10: 0.178, AP: 0.175, loss: 0.445\n",
      "iter:    31000, P10: 0.185, AP: 0.177, loss: 0.382\n",
      "2019-03-13 15:01:30,403 [MainThread  ] [WARNI]  max_dips reached, stopped at 32000 iterations.\n",
      "2019-03-13 15:01:30,845 [MainThread  ] [INFO ]  Validating fold 2 ...\n",
      "iter:        0, P10: 0.161, AP: 0.170, loss: 0.000\n",
      "iter:     1000, P10: 0.177, AP: 0.176, loss: 1.310\n",
      "iter:     2000, P10: 0.175, AP: 0.179, loss: 1.097\n",
      "iter:     3000, P10: 0.170, AP: 0.178, loss: 0.965\n",
      "iter:     4000, P10: 0.168, AP: 0.177, loss: 0.969\n",
      "iter:     5000, P10: 0.167, AP: 0.174, loss: 0.865\n",
      "iter:     6000, P10: 0.160, AP: 0.173, loss: 0.752\n",
      "iter:     7000, P10: 0.162, AP: 0.173, loss: 0.802\n",
      "iter:     8000, P10: 0.161, AP: 0.172, loss: 0.685\n",
      "iter:     9000, P10: 0.160, AP: 0.171, loss: 0.644\n",
      "iter:    10000, P10: 0.156, AP: 0.172, loss: 0.643\n",
      "iter:    11000, P10: 0.157, AP: 0.172, loss: 0.586\n",
      "iter:    12000, P10: 0.160, AP: 0.170, loss: 0.600\n",
      "iter:    13000, P10: 0.165, AP: 0.172, loss: 0.572\n",
      "iter:    14000, P10: 0.169, AP: 0.171, loss: 0.569\n",
      "iter:    15000, P10: 0.164, AP: 0.174, loss: 0.581\n",
      "iter:    16000, P10: 0.169, AP: 0.172, loss: 0.538\n",
      "iter:    17000, P10: 0.151, AP: 0.169, loss: 0.541\n",
      "iter:    18000, P10: 0.160, AP: 0.169, loss: 0.563\n",
      "iter:    19000, P10: 0.151, AP: 0.169, loss: 0.536\n",
      "iter:    20000, P10: 0.157, AP: 0.170, loss: 0.497\n",
      "iter:    21000, P10: 0.161, AP: 0.171, loss: 0.481\n",
      "2019-03-13 15:01:52,352 [MainThread  ] [WARNI]  max_dips reached, stopped at 22000 iterations.\n",
      "2019-03-13 15:01:52,601 [MainThread  ] [INFO ]  Validating fold 3 ...\n",
      "iter:        0, P10: 0.172, AP: 0.179, loss: 0.000\n",
      "iter:     1000, P10: 0.162, AP: 0.176, loss: 1.270\n",
      "iter:     2000, P10: 0.163, AP: 0.175, loss: 1.149\n",
      "iter:     3000, P10: 0.180, AP: 0.180, loss: 0.973\n",
      "iter:     4000, P10: 0.174, AP: 0.179, loss: 0.915\n",
      "iter:     5000, P10: 0.171, AP: 0.178, loss: 0.824\n",
      "iter:     6000, P10: 0.171, AP: 0.178, loss: 0.797\n",
      "iter:     7000, P10: 0.171, AP: 0.176, loss: 0.734\n",
      "iter:     8000, P10: 0.160, AP: 0.175, loss: 0.745\n",
      "iter:     9000, P10: 0.172, AP: 0.175, loss: 0.661\n",
      "iter:    10000, P10: 0.170, AP: 0.175, loss: 0.630\n",
      "iter:    11000, P10: 0.172, AP: 0.176, loss: 0.591\n",
      "iter:    12000, P10: 0.162, AP: 0.174, loss: 0.600\n",
      "iter:    13000, P10: 0.175, AP: 0.175, loss: 0.600\n",
      "iter:    14000, P10: 0.175, AP: 0.176, loss: 0.506\n",
      "iter:    15000, P10: 0.166, AP: 0.176, loss: 0.563\n",
      "iter:    16000, P10: 0.160, AP: 0.173, loss: 0.579\n"
     ]
    }
   ],
   "source": [
    "cv('cal500', 512, n_folds=3, method='pamir', valid_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation results including retrieval method parameters are written to a JSON file. For each dataset three separate result files for mean average precision (MAP), precision-at-*k*, and precision-at-10 as a function of relevant training examples are written to disk. Here are the mean average precision values of the last cross-validation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from cbar.settings import RESULTS_DIR\n",
    "\n",
    "results = json.load(open(os.path.join(RESULTS_DIR, 'cal500_mean_ap.json')))\n",
    "results[list(results.keys())[-1]]['precision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start cross-validation with the CLI\n",
    "\n",
    "This package comes with a simple CLI which makes it easy to start cross-validation experiments from the command line. The CLI enables you to specify a dataset and a retrieval method as well as additional options in one line.\n",
    "\n",
    "To start an experiment on the CAL500 dataset with the LORETA retrieval method, use the following command.\n",
    "\n",
    "```\n",
    "$ cbar crossval --dataset cal500 loreta \n",
    "```\n",
    "\n",
    "This simple command uses all the default parameters for LORETA but you can specify all parameters as arguments to the `loreta` command. To see the available options for the `loreta` command, ask for help like this.\n",
    "\n",
    "```\n",
    "$ cbar crossval loreta --help\n",
    "Usage: cbar crossval loreta [OPTIONS]\n",
    "\n",
    "Options:\n",
    "  -n, --max-iter INTEGER        Maximum number of iterations\n",
    "  -i, --valid-interval INTEGER  Rank of parameter matrix W\n",
    "  -k INTEGER                    Rank of parameter matrix W\n",
    "  --n0 FLOAT                    Step size parameter 1\n",
    "  --n1 FLOAT                    Step size parameter 2\n",
    "  -t, --rank-thresh FLOAT       Threshold for early stopping\n",
    "  -l, --lambda FLOAT            Regularization constant\n",
    "  --loss [warp|auc]             Loss function\n",
    "  -d, --max-dips INTEGER        Maximum number of dips\n",
    "  -v, --verbose                 Verbosity\n",
    "  --help                        Show this message and exit.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
