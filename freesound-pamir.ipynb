{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Quickstart guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to build a simple content-based audio retrieval model and evaluate the retrieval accuracy on a small song dataset, CAL500. This dataset consists of 502 western pop songs, performed by 499 unique artists. Each song is tagged by at least three people using a standard survey and a fixed tag vocabulary of 174 musical concepts.\n",
    "\n",
    "This package includes a loading utility for getting and processing this dataset, which makes loading quite easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbar.datasets.freesound import load_freesound\n",
    "\n",
    "X, Y = load_freesound(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `fetch_cal500()` initally downloads the CAL500 dataset to a subfolder of your home directory. You can specify a different location using the `data_home` parameter (`fetch_cal500(data_home='path')`). Subsequents calls simply load the dataset.\n",
    "\n",
    "The raw dataset consists of about 10,000 39-dimensional features vectors\n",
    "per minute of audio content which were created by\n",
    "\n",
    "1. Sliding a half-overlapping short-time window of 12 milliseconds over each song's waveform data.\n",
    "2. Extracting the 13 mel-frequency cepstral coefficients.\n",
    "3. Appending the instantaneous first-order and second-order derivatives.\n",
    "\n",
    "Each song is, then, represented by exactly 10,000 randomly subsampled, real-valued feature vectors as a *bag-of-frames*. The *bag-of-frames* features are further processed into one *k*-dimensional feature vector by encoding the feature vectors using a codebook and pooling them into one compact vector.\n",
    "\n",
    "Specifically, *k*-means is used to cluster all frame vectors into *k* clusters. The resulting cluster centers correspond to the codewords in the codebook. Each frame vector is assigned to its closest cluster center and a song represented as the counts of frames assigned to each of the *k* cluster centers.\n",
    "\n",
    "By default, `fetch_cal500()` uses a codebook size of 512 but this size is easily modified with the `codebook_size` parameter (`fetch_cal500(codebook_size=1024)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((227085, 512), (227085,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training data and test data, fit the model on the training data, and evaluate it on the test data. Import and instantiate the model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbar.pamir import PAMIR\n",
    "\n",
    "model = PAMIR(valid_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then split the data and fit the model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:        0, P10: 0.002, AP: 0.002, loss: 0.000\n",
      "iter:     1000, P10: 0.002, AP: 0.002, loss: 1.020\n",
      "iter:     2000, P10: 0.002, AP: 0.002, loss: 1.003\n",
      "iter:     3000, P10: 0.002, AP: 0.002, loss: 1.009\n",
      "iter:     4000, P10: 0.002, AP: 0.002, loss: 1.033\n",
      "iter:     5000, P10: 0.002, AP: 0.002, loss: 1.009\n",
      "iter:     6000, P10: 0.002, AP: 0.002, loss: 0.997\n",
      "iter:     7000, P10: 0.002, AP: 0.002, loss: 0.982\n",
      "iter:     8000, P10: 0.002, AP: 0.002, loss: 1.014\n",
      "iter:     9000, P10: 0.002, AP: 0.002, loss: 1.022\n",
      "iter:    10000, P10: 0.002, AP: 0.002, loss: 1.010\n",
      "iter:    11000, P10: 0.002, AP: 0.002, loss: 1.019\n",
      "iter:    12000, P10: 0.002, AP: 0.002, loss: 0.964\n",
      "iter:    13000, P10: 0.002, AP: 0.002, loss: 0.992\n",
      "iter:    14000, P10: 0.002, AP: 0.002, loss: 1.036\n",
      "iter:    15000, P10: 0.002, AP: 0.002, loss: 1.020\n",
      "iter:    16000, P10: 0.002, AP: 0.002, loss: 1.001\n",
      "iter:    17000, P10: 0.002, AP: 0.002, loss: 0.967\n",
      "iter:    18000, P10: 0.002, AP: 0.002, loss: 0.983\n",
      "iter:    19000, P10: 0.002, AP: 0.002, loss: 0.983\n",
      "iter:    20000, P10: 0.002, AP: 0.002, loss: 0.998\n",
      "iter:    21000, P10: 0.002, AP: 0.002, loss: 0.952\n",
      "iter:    22000, P10: 0.002, AP: 0.002, loss: 1.001\n",
      "iter:    23000, P10: 0.002, AP: 0.002, loss: 1.019\n",
      "iter:    24000, P10: 0.002, AP: 0.002, loss: 0.964\n",
      "iter:    25000, P10: 0.002, AP: 0.002, loss: 0.978\n",
      "iter:    26000, P10: 0.002, AP: 0.002, loss: 0.992\n",
      "iter:    27000, P10: 0.002, AP: 0.002, loss: 1.009\n",
      "iter:    28000, P10: 0.002, AP: 0.002, loss: 0.961\n",
      "iter:    29000, P10: 0.002, AP: 0.002, loss: 0.978\n",
      "iter:    30000, P10: 0.002, AP: 0.002, loss: 0.981\n",
      "iter:    31000, P10: 0.002, AP: 0.002, loss: 0.980\n",
      "iter:    32000, P10: 0.002, AP: 0.002, loss: 0.950\n",
      "iter:    33000, P10: 0.002, AP: 0.002, loss: 0.966\n",
      "iter:    34000, P10: 0.002, AP: 0.002, loss: 0.975\n",
      "iter:    35000, P10: 0.002, AP: 0.002, loss: 0.989\n",
      "iter:    36000, P10: 0.002, AP: 0.002, loss: 0.943\n",
      "iter:    37000, P10: 0.002, AP: 0.002, loss: 0.956\n",
      "iter:    38000, P10: 0.002, AP: 0.002, loss: 0.950\n",
      "iter:    39000, P10: 0.002, AP: 0.002, loss: 0.969\n",
      "iter:    40000, P10: 0.002, AP: 0.002, loss: 0.977\n",
      "iter:    41000, P10: 0.002, AP: 0.002, loss: 0.945\n",
      "iter:    42000, P10: 0.002, AP: 0.002, loss: 0.960\n",
      "iter:    43000, P10: 0.002, AP: 0.002, loss: 0.941\n",
      "iter:    44000, P10: 0.002, AP: 0.002, loss: 0.930\n",
      "iter:    45000, P10: 0.002, AP: 0.002, loss: 0.965\n",
      "iter:    46000, P10: 0.002, AP: 0.002, loss: 0.969\n",
      "iter:    47000, P10: 0.002, AP: 0.002, loss: 0.945\n",
      "iter:    48000, P10: 0.002, AP: 0.002, loss: 0.946\n",
      "iter:    49000, P10: 0.002, AP: 0.002, loss: 0.941\n",
      "iter:    50000, P10: 0.002, AP: 0.002, loss: 0.929\n",
      "iter:    51000, P10: 0.002, AP: 0.002, loss: 0.940\n",
      "iter:    52000, P10: 0.002, AP: 0.002, loss: 0.942\n",
      "iter:    53000, P10: 0.002, AP: 0.002, loss: 0.936\n",
      "iter:    54000, P10: 0.002, AP: 0.002, loss: 0.962\n",
      "iter:    55000, P10: 0.002, AP: 0.002, loss: 0.942\n",
      "iter:    56000, P10: 0.002, AP: 0.002, loss: 0.937\n",
      "iter:    57000, P10: 0.002, AP: 0.002, loss: 0.948\n",
      "iter:    58000, P10: 0.002, AP: 0.002, loss: 0.921\n",
      "iter:    59000, P10: 0.002, AP: 0.002, loss: 0.922\n",
      "iter:    60000, P10: 0.002, AP: 0.002, loss: 0.914\n",
      "iter:    61000, P10: 0.002, AP: 0.002, loss: 0.920\n",
      "iter:    62000, P10: 0.002, AP: 0.002, loss: 0.964\n",
      "iter:    63000, P10: 0.002, AP: 0.002, loss: 0.931\n",
      "iter:    64000, P10: 0.002, AP: 0.002, loss: 0.920\n",
      "iter:    65000, P10: 0.002, AP: 0.002, loss: 0.951\n",
      "iter:    66000, P10: 0.002, AP: 0.002, loss: 0.932\n",
      "iter:    67000, P10: 0.002, AP: 0.002, loss: 0.940\n",
      "iter:    68000, P10: 0.002, AP: 0.002, loss: 0.938\n",
      "iter:    69000, P10: 0.002, AP: 0.002, loss: 0.951\n",
      "iter:    70000, P10: 0.002, AP: 0.002, loss: 0.959\n",
      "iter:    71000, P10: 0.002, AP: 0.002, loss: 0.929\n",
      "iter:    72000, P10: 0.002, AP: 0.002, loss: 0.903\n",
      "iter:    73000, P10: 0.002, AP: 0.002, loss: 0.924\n",
      "iter:    74000, P10: 0.002, AP: 0.002, loss: 0.903\n",
      "iter:    75000, P10: 0.002, AP: 0.002, loss: 0.932\n",
      "iter:    76000, P10: 0.002, AP: 0.002, loss: 0.927\n",
      "iter:    77000, P10: 0.002, AP: 0.002, loss: 0.909\n",
      "iter:    78000, P10: 0.002, AP: 0.002, loss: 0.929\n",
      "iter:    79000, P10: 0.002, AP: 0.002, loss: 0.932\n",
      "iter:    80000, P10: 0.002, AP: 0.002, loss: 0.927\n",
      "iter:    81000, P10: 0.002, AP: 0.002, loss: 0.924\n",
      "iter:    82000, P10: 0.002, AP: 0.002, loss: 0.930\n",
      "iter:    83000, P10: 0.002, AP: 0.002, loss: 0.956\n",
      "iter:    84000, P10: 0.002, AP: 0.002, loss: 0.937\n",
      "iter:    85000, P10: 0.002, AP: 0.002, loss: 0.918\n",
      "iter:    86000, P10: 0.002, AP: 0.002, loss: 0.906\n",
      "iter:    87000, P10: 0.002, AP: 0.002, loss: 0.954\n",
      "iter:    88000, P10: 0.002, AP: 0.002, loss: 0.921\n",
      "iter:    89000, P10: 0.002, AP: 0.002, loss: 0.887\n",
      "iter:    90000, P10: 0.002, AP: 0.002, loss: 0.929\n",
      "iter:    91000, P10: 0.002, AP: 0.002, loss: 0.887\n",
      "iter:    92000, P10: 0.002, AP: 0.002, loss: 0.927\n",
      "iter:    93000, P10: 0.002, AP: 0.002, loss: 0.923\n",
      "iter:    94000, P10: 0.002, AP: 0.002, loss: 0.889\n",
      "iter:    96000, P10: 0.002, AP: 0.002, loss: 0.931\n",
      "iter:    97000, P10: 0.002, AP: 0.002, loss: 0.924\n",
      "iter:    98000, P10: 0.002, AP: 0.002, loss: 0.893\n",
      "iter:    99000, P10: 0.002, AP: 0.002, loss: 0.906\n",
      "CPU times: user 7h 17min 3s, sys: 15h 59min 32s, total: 23h 16min 36s\n",
      "Wall time: 1h 27min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cbar.pamir.PAMIR(max_iter=100000, C=1.0, valid_interval=1000, max_dips=20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cbar.cross_validation import train_test_split_plus\n",
    "\n",
    "(X_train, X_test,\n",
    " Y_train, Y_test,\n",
    " Q_vec, weights) = train_test_split_plus(X, Y)\n",
    "\n",
    "%time model.fit(X_train, Y_train, Q_vec, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, predict the scores for each query with all songs. Ordering the songs from highest to lowest score corresponds to the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_score = model.predict(Q_vec, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: [0.0011557353366079169],\n",
       "             2: [0.001878069921987865],\n",
       "             3: [0.0017336030049118752],\n",
       "             4: [0.0017336030049118754],\n",
       "             5: [0.001906963305403063],\n",
       "             6: [0.002167003756139844],\n",
       "             7: [0.0020638131010855655],\n",
       "             8: [0.0020225368390638545],\n",
       "             9: [0.0020024719894699674],\n",
       "             10: [0.002022536839063855],\n",
       "             11: [0.0020120301541856006],\n",
       "             12: [0.0019813856566240273],\n",
       "             13: [0.0020036113361741794],\n",
       "             14: [0.002010279040039225],\n",
       "             15: [0.0020921436263822858],\n",
       "             16: [0.0021487265022370485],\n",
       "             17: [0.002185905488249252],\n",
       "             18: [0.0022268613871261867],\n",
       "             19: [0.0022681465234732382],\n",
       "             20: [0.0022141615176185262]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cbar.evaluation import Evaluator\n",
    "from cbar.utils import make_relevance_matrix\n",
    "\n",
    "n_relevant = make_relevance_matrix(Q_vec, Y_train).sum(axis=1)\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.eval(Q_vec, weights, Y_score, Y_test, n_relevant)\n",
    "evaluator.prec_at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "The `cv` function in the `cross_validation` module offers an easy way to evaluate a retrieval method on multiple splits of the data. Let's run the same experiment on three folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbar.cross_validation import cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-10 04:59:32,138 [MainThread  ] [INFO ]  Running CV with 3 folds ...\n",
      "2019-04-10 04:59:40,756 [MainThread  ] [INFO ]  Validating fold 0 ...\n",
      "iter:        0, P10: 0.002, AP: 0.002, loss: 0.000\n",
      "iter:     1000, P10: 0.002, AP: 0.002, loss: 1.001\n",
      "iter:     2000, P10: 0.002, AP: 0.002, loss: 0.999\n",
      "iter:     3000, P10: 0.002, AP: 0.002, loss: 1.014\n",
      "iter:     4000, P10: 0.002, AP: 0.002, loss: 0.994\n",
      "iter:     5000, P10: 0.002, AP: 0.002, loss: 1.007\n",
      "iter:     6000, P10: 0.002, AP: 0.002, loss: 1.010\n",
      "iter:     7000, P10: 0.002, AP: 0.002, loss: 1.027\n",
      "iter:     8000, P10: 0.002, AP: 0.002, loss: 0.989\n",
      "iter:     9000, P10: 0.002, AP: 0.002, loss: 1.023\n",
      "iter:    10000, P10: 0.002, AP: 0.002, loss: 1.010\n"
     ]
    }
   ],
   "source": [
    "%time cv('freesound', 512, n_folds=3, method='pamir', valid_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation results including retrieval method parameters are written to a JSON file. For each dataset three separate result files for mean average precision (MAP), precision-at-*k*, and precision-at-10 as a function of relevant training examples are written to disk. Here are the mean average precision values of the last cross-validation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from cbar.settings import RESULTS_DIR\n",
    "\n",
    "results = json.load(open(os.path.join(RESULTS_DIR, 'freesound_mean_ap.json')))\n",
    "results[list(results.keys())[-1]]['precision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start cross-validation with the CLI\n",
    "\n",
    "This package comes with a simple CLI which makes it easy to start cross-validation experiments from the command line. The CLI enables you to specify a dataset and a retrieval method as well as additional options in one line.\n",
    "\n",
    "To start an experiment on the CAL500 dataset with the LORETA retrieval method, use the following command.\n",
    "\n",
    "```\n",
    "$ cbar crossval --dataset cal500 loreta \n",
    "```\n",
    "\n",
    "This simple command uses all the default parameters for LORETA but you can specify all parameters as arguments to the `loreta` command. To see the available options for the `loreta` command, ask for help like this.\n",
    "\n",
    "```\n",
    "$ cbar crossval loreta --help\n",
    "Usage: cbar crossval loreta [OPTIONS]\n",
    "\n",
    "Options:\n",
    "  -n, --max-iter INTEGER        Maximum number of iterations\n",
    "  -i, --valid-interval INTEGER  Rank of parameter matrix W\n",
    "  -k INTEGER                    Rank of parameter matrix W\n",
    "  --n0 FLOAT                    Step size parameter 1\n",
    "  --n1 FLOAT                    Step size parameter 2\n",
    "  -t, --rank-thresh FLOAT       Threshold for early stopping\n",
    "  -l, --lambda FLOAT            Regularization constant\n",
    "  --loss [warp|auc]             Loss function\n",
    "  -d, --max-dips INTEGER        Maximum number of dips\n",
    "  -v, --verbose                 Verbosity\n",
    "  --help                        Show this message and exit.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
